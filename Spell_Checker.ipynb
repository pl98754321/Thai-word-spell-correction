{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spell_Checker_total.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# initial"
      ],
      "metadata": {
        "id": "6isHM4nBL6PB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kLM19vRJ_p3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import lib"
      ],
      "metadata": {
        "id": "iYsB1Y7iMM3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pythainlp -qq\n",
        "!pip install epitran -qq\n",
        "!pip install sklearn_crfsuite -qq\n",
        "!pip install tensorflow deepcut -qq\n",
        "!pip install attacut -qq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqB1IMKxMKq2",
        "outputId": "d6699bb1-13fe-4bfb-a607-b8b96bdb8129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 11.5 MB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 148 kB 5.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 44.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 73 kB 1.2 MB/s \n",
            "\u001b[?25h  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 743 kB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 462 kB 39.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 473 kB 64.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 35.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 87 kB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 45 kB 2.5 MB/s \n",
            "\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Data management\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import cv2\n",
        "import re\n",
        "import string\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "from pythainlp import sent_tokenize, word_tokenize\n",
        "\n",
        "\n",
        "#data visualize\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "Kq-n1OddMLmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get corpus"
      ],
      "metadata": {
        "id": "ge6doroXMPGK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "class"
      ],
      "metadata": {
        "id": "jOfy_3F9MVRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pythainlp import sent_tokenize, word_tokenize\n",
        "from pythainlp import thai_digits, thai_letters\n",
        "from pythainlp.spell import NorvigSpellChecker\n",
        "from pythainlp.corpus import download , get_corpus_path , get_corpus"
      ],
      "metadata": {
        "id": "bx6nAzz-MUpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Corpus():\n",
        "  \n",
        "  def __init__(self,dict_word_fre):\n",
        "    self.dictt = dict_word_fre\n",
        "    self.word = list(dict_word_fre.keys())\n",
        "    self.fre = list(dict_word_fre.values())\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.word)\n",
        "\n",
        "  def __getitem__(self,word):\n",
        "    return self.dictt[word]\n",
        "\n",
        "class Corpus_from_dow(Corpus):\n",
        "  def __init__(self,name):\n",
        "    dict_cor = self.dow(name)\n",
        "    Corpus.__init__(self,dict_cor)\n",
        "  def dow(self,name):\n",
        "    download(name)\n",
        "    path = get_corpus_path(name)\n",
        "    cor = get_corpus(path)\n",
        "    word = [\"\".join(x.split(\"\\t\")[:-1]) for x in cor]\n",
        "    fre = [int(x.split(\"\\t\")[-1]) for x in cor]\n",
        "    dict_cor = dict(zip(word, fre))\n",
        "    return dict_cor"
      ],
      "metadata": {
        "id": "f05ldyeUMTPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "uni bi tri"
      ],
      "metadata": {
        "id": "I3U21CfbMWOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checker = NorvigSpellChecker()  # use default filter (remove any word with number or non-Thai character)"
      ],
      "metadata": {
        "id": "0krJYJecMZCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigrame = Corpus(dict(checker.dictionary()))\n",
        "trigrame = Corpus_from_dow(\"tnc_trigram_word_freqs\")\n",
        "bigrame = Corpus_from_dow(\"tnc_bigram_word_freqs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOa0R1d4MbF6",
        "outputId": "cddfdff3-4fc9-4d2c-9d0b-10ec8eea4cdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus: tnc_trigram_word_freqs\n",
            "- Downloading: tnc_trigram_word_freqs 2017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 151914848/151914848 [00:01<00:00, 93217584.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus: tnc_bigram_word_freqs\n",
            "- Downloading: tnc_bigram_word_freqs 2017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 42200711/42200711 [00:00<00:00, 112318713.26it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main()"
      ],
      "metadata": {
        "id": "SCH-1XinL3OY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Paragraph --> sentense"
      ],
      "metadata": {
        "id": "_j3R5O__QD4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class candidater():\n",
        "  def edits1(self,word):\n",
        "    \"All edits that are one edit away from `word`.\"    \n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    # transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    #replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    # inserts    = [L + c + R               for L, R in splits for c in letters]   \n",
        "    return set(deletes)#set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "  def delet2dict(self,dict_new,text):   \n",
        "    for delet1 in self.edits1(text):\n",
        "      if dict_new.get(delet1) is None:\n",
        "        dict_new.update({\n",
        "            delet1:{text}\n",
        "            })\n",
        "      else:\n",
        "        dict_new.get(delet1).add(text)\n",
        "\n",
        "  def __init__(self,unigrame):\n",
        "    self.unidict = unigrame.dictt\n",
        "    uniword = unigrame.word    \n",
        "    dict_new = {}\n",
        "    for text in uniword:     \n",
        "      self.delet2dict(dict_new,text)\n",
        "    self.dict_new = dict_new\n",
        "\n",
        "  def candidate(self,word):\n",
        "    set_candidate = set()\n",
        "\n",
        "    if word in self.dict_new.keys():\n",
        "      set_candidate.update(self.dict_new[word])\n",
        "      \n",
        "    del_set = self.edits1(word)\n",
        "\n",
        "    for word_del in del_set:\n",
        "      \n",
        "      if word_del in self.unidict.keys():\n",
        "        set_candidate.add(word_del)\n",
        "\n",
        "      if word_del in self.dict_new.keys():\n",
        "        set_candidate.update(self.dict_new[word_del])\n",
        "    if len(word) == 1 and word != \" \":\n",
        "      set_candidate.add(\"\")\n",
        "    return set_candidate"
      ],
      "metadata": {
        "id": "YKud8Y4nuG9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candit = candidater(unigrame)"
      ],
      "metadata": {
        "id": "Q1dJcP9huNBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "P'Mick\n",
        "input  :: Paragraph\n",
        "output :: Sentense\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "SKD31bbpPVhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#combine function for find correct words\n",
        "def correct_word_s(input_words):  \n",
        "  #2. find possible word for each word\n",
        "  word__wrong_list = input_words\n",
        "\n",
        "  record_possible_word =[]\n",
        "  \n",
        "  \n",
        "  for word in word__wrong_list:\n",
        "    # each_word_list =[]\n",
        "    each_word_list = candit.candidate(word)\n",
        "    record_possible_word.append(each_word_list)  \n",
        "\n",
        "  #3.combine each possible words to trigram word\n",
        "\n",
        "  if len(record_possible_word) != 0:\n",
        "    all_edit_words =[]\n",
        "    for i in range(len(word__wrong_list)):\n",
        "      for item in record_possible_word[i]:\n",
        "        if i == 0:\n",
        "          new_word = item + word__wrong_list[1] + word__wrong_list[2]\n",
        "        elif i == 1:\n",
        "          new_word = word__wrong_list[0] + item + word__wrong_list[2]\n",
        "        else:\n",
        "          new_word = word__wrong_list[0] + word__wrong_list[1] + item\n",
        "        all_edit_words.append(new_word)\n",
        "        \n",
        "  else:\n",
        "    print('no record possible word')\n",
        "\n",
        "  #4. loop edit word in dict again to find most freq(prob)\n",
        "\n",
        "  selected_words=[]\n",
        "  freq_words=[]\n",
        "  #after augment may be reduce to 1 words (unigrame dict)\n",
        "  for item in all_edit_words:  \n",
        "    if item in unigrame.dictt.keys():\n",
        "      freq_word = unigrame.dictt[item]\n",
        "      selected_words.append(item)\n",
        "      freq_words.append(int(freq_word))\n",
        "    if item in bigrame.dictt.keys():\n",
        "      freq_word = bigrame.dictt[item]\n",
        "      selected_words.append(item)\n",
        "      freq_words.append(int(freq_word))\n",
        "    if item in trigrame.dictt.keys():\n",
        "      freq_word = trigrame.dictt[item]\n",
        "      selected_words.append(item)\n",
        "      freq_words.append(int(freq_word))            \n",
        "    \n",
        "\n",
        "  #select the most freq\n",
        "  try:\n",
        "    correct_word = selected_words[np.argmax(freq_words)]\n",
        "    return correct_word\n",
        "  except: #may be correct but wrong cut that why we cannot find it in dict   \n",
        "    return \"\".join(input_words)"
      ],
      "metadata": {
        "id": "omYxKCrPtzHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pythainlp -qq\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "\n",
        "N = sum(trigrame.dictt.values())\n",
        "\n",
        "def is_overlapped(a,b):\n",
        "  if a[0] > b[0]: a,b = b,a\n",
        "  if a[1] > b[0]: return True\n",
        "  return False\n",
        "\n",
        "def add_prop(sentence,candidates):\n",
        "  sentence = list(sentence)\n",
        "  window_size = 3\n",
        "  result = []\n",
        "  for candidate in candidates:\n",
        "    start, end = candidate[\"start\"], candidate[\"end\"]\n",
        "    sen = sentence[:]\n",
        "    sen[start:end] = list(candidate[\"new_word\"])\n",
        "    segs = word_tokenize(''.join(sen), engine='attacut')\n",
        "    prod = 1\n",
        "    for i in range(len(segs) - window_size+1):\n",
        "      gram = segs[i:i+window_size]\n",
        "      gram = ''.join(gram)\n",
        "      if gram in trigrame.dictt:\n",
        "        ele = trigrame.dictt[gram]\n",
        "      else:\n",
        "        ele = 0\n",
        "      prod += ele\n",
        "    candidate.update({\"prop\":prod})\n",
        "    result.append(candidate)\n",
        "  return result\n",
        "    \n",
        "def remove_overlap(sentence, candidates):\n",
        "  candidates_prop = add_prop(sentence,candidates)\n",
        "  # candi = sorted(candidates_prop, key=lambda x: x['prop'])\n",
        "  candi = candidates_prop\n",
        "  sen = sentence[:]\n",
        "  i = 0\n",
        "  list_candidate = []\n",
        "  while i < len(candi):\n",
        "    a,b,c = None,None,None\n",
        "    a = [candi[i][\"start\"], candi[i][\"end\"]]\n",
        "    if i + 1 < len(candi):\n",
        "      b = [candi[i+1][\"start\"], candi[i+1][\"end\"]]\n",
        "    if i + 2 < len(candi):\n",
        "      c = [candi[i+2][\"start\"], candi[i+2][\"end\"]]\n",
        "    \n",
        "    if b is None and c is None:\n",
        "      list_candidate.append(candi[i])\n",
        "      break\n",
        "    \n",
        "    if b is not None and c is None:\n",
        "      if is_overlapped(a,b):\n",
        "        if candi[i]['prop'] >= candi[i+1]['prop']:\n",
        "          list_candidate.append(candi[i])\n",
        "        else:\n",
        "          list_candidate.append(candi[i+1])\n",
        "      else:\n",
        "        list_candidate.append(candi[i])\n",
        "        list_candidate.append(candi[i+1])\n",
        "      break\n",
        "\n",
        "    else:\n",
        "      if is_overlapped(a,b):\n",
        "        if is_overlapped(a,c):\n",
        "          agmax = np.argmax([candi[i]['prop'], candi[i+1]['prop'], candi[i+2]['prop']])\n",
        "          list_candidate.append(candi[agmax])\n",
        "          i += 3\n",
        "        else:\n",
        "          if candi[i]['prop'] >= candi[i+1]['prop']:\n",
        "            list_candidate.append(candi[i])\n",
        "          else:\n",
        "            list_candidate.append(candi[i+1])\n",
        "          i += 2\n",
        "      else:\n",
        "        list_candidate.append(candi[i])\n",
        "        list_candidate.append(candi[i+1])\n",
        "        list_candidate.append(candi[i+2])\n",
        "        i += 3\n",
        "\n",
        "  def get_prob(list_dict):\n",
        "    return list_dict[\"prop\"]\n",
        "  \n",
        "  ans_list = list()\n",
        "  count_idx = 0\n",
        "  list_candidate.sort(reverse=True,key=get_prob)\n",
        "  return list_candidate"
      ],
      "metadata": {
        "id": "Xw5YYdfLTewE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Pmick(paragraph):\n",
        "  def prepare_index(ans_list, length_acc_list):\n",
        "    for idx, sen in enumerate(ans_list):\n",
        "      if idx != 0:\n",
        "        for each in sen:\n",
        "          each['start']  += length_acc_list[idx-1]\n",
        "          each['end']  += length_acc_list[idx-1]\n",
        "          \n",
        "    return ans_list\n",
        "  \n",
        "  ans_list = list()\n",
        "  count_idx = 0\n",
        "\n",
        "  # Find accumulative index in pg\n",
        "  sentence_list = sent_tokenize(paragraph)\n",
        "  length_acc_list = list()\n",
        "  length_acc = 0\n",
        "  for i in sentence_list:\n",
        "    length_acc += len(i)\n",
        "    length_acc_list.append(length_acc)\n",
        "\n",
        "\n",
        "  for text in sentence_list:\n",
        "    \n",
        "    word_deepcut =  word_tokenize(text, engine=\"attacut\") #deep or atta\n",
        "\n",
        "    full_word=[]\n",
        "    Wrong_word=[]\n",
        "\n",
        "    #index\n",
        "    start_idx_list=[]\n",
        "    end_idx_list=[]\n",
        "\n",
        "    #step1. \n",
        "    count = 1\n",
        "\n",
        "    for idx in range(len(word_deepcut)):\n",
        "\n",
        "      if idx == 0: #only first index to add </s> for first 2 words\n",
        "        word=word_deepcut[idx:idx+2]\n",
        "\n",
        "        start_idx_list.append(0) #add value 0\n",
        "        count_word=''.join(word)    \n",
        "        end_idx_list.append(len(count_word))   #add start+len  \n",
        "\n",
        "        word.insert(0,'<s/>')\n",
        "        full_word.append(word)\n",
        "\n",
        "        \n",
        "      if idx == len(word_deepcut)-2:\n",
        "        word=word_deepcut[idx:idx+2]\n",
        "        count_word=''.join(word)    \n",
        "        \n",
        "        start_idx_list.append(count-1)\n",
        "        count+=len(count_word)\n",
        "        end_idx_list.append(count-1)\n",
        "        word.append('<s/>')\n",
        "        full_word.append(word)\n",
        "        break\n",
        "\n",
        "      else:\n",
        "        word=word_deepcut[idx:idx+3]\n",
        "        count_word=''.join(word)    \n",
        "        start_idx_list.append(count-1)    \n",
        "        end_idx_list.append(count+len(count_word)-1)\n",
        "        count+=len(word[0]) #next start index\n",
        "\n",
        "      full_word.append(word)\n",
        "      start_idx_list[1] = 0 #direct apply\n",
        "\n",
        "\n",
        "    #step2.\n",
        "    wrong_words =[]\n",
        "    correct_words =[]\n",
        "    wrong_word_combine=[]\n",
        "    selected_start_index=[]\n",
        "    selected_end_index=[]\n",
        "\n",
        "    for i in range(len(full_word)):\n",
        "      tri_word=\"\".join(full_word[i])\n",
        "      if tri_word in trigrame.word:\n",
        "        # correct_words.append(item)\n",
        "        pass   \n",
        "      else:    \n",
        "        wrong_words.append(full_word[i])\n",
        "        wrong_word_combine.append(tri_word)\n",
        "        selected_start_index.append(start_idx_list[i])\n",
        "        selected_end_index.append(end_idx_list[i])\n",
        "\n",
        "\n",
        "    edit_word_list=[]\n",
        "\n",
        "    #find correct word\n",
        "    for item in wrong_words:\n",
        "      correct_one = correct_word_s(item)\n",
        "      edit_word_list.append(correct_one)\n",
        "\n",
        "    #find dict list\n",
        "    result =[]\n",
        "    for i in range(len(edit_word_list)):\n",
        "      edit_word ={}\n",
        "      if wrong_word_combine[i] != edit_word_list[i]:    \n",
        "        edit_word[\"start\"] = selected_start_index[i]\n",
        "        edit_word[\"end\"] = selected_end_index[i]\n",
        "        edit_word[\"old_word\"] = wrong_word_combine[i]\n",
        "        edit_word[\"new_word\"] = edit_word_list[i]\n",
        "        result.append(edit_word)\n",
        "\n",
        "    ans_list.append(result)\n",
        "  ans_list = [remove_overlap(sentence_list[index],ans_list[index])[:2] for index in  range(len(ans_list))]\n",
        "  ans = prepare_index(ans_list, length_acc_list)\n",
        "  return ans"
      ],
      "metadata": {
        "id": "krfISaF5tZZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentense = \"ทำไมครที่รักสุดท้ายต้องจากกัน ทำไมรเป็นอย่างน้นโผมยังไม่เข้าใจ หรือเคยทำกรรมไรไว้ เคยไปทำิอะไรใครที่ไหน\""
      ],
      "metadata": {
        "id": "UJyOEQ9ffHTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Pmick(sentense)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umrlClcWwx7n",
        "outputId": "38502f27-b24d-44de-8786-f57c5387545e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ALERT!!!!!!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[{'end': 6,\n",
              "   'new_word': '<s/>ทำไมคน',\n",
              "   'old_word': '<s/>ทำไมคร',\n",
              "   'prop': 54,\n",
              "   'start': 0}],\n",
              " [{'end': 35,\n",
              "   'new_word': '<s/>ทำไม',\n",
              "   'old_word': '<s/>ทำไมร',\n",
              "   'prop': 238,\n",
              "   'start': 30}],\n",
              " [{'end': 90,\n",
              "   'new_word': 'เคยไปทำ',\n",
              "   'old_word': 'เคยไปทำิ',\n",
              "   'prop': 555,\n",
              "   'start': 82}]]"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OwHRWX-vyzyX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}